# Audio Generator

## Overview
The Audio Generator takes generated shots from the Shot Creator and converts them into high-quality voice narration audio files. This system uses the Chatterbox TTS engine to synthesize speech with fine-grained control over voice characteristics, emotional expression, and speech patterns for each shot.

## Input Data Model

The Audio Generator processes Shot objects that were generated by the Shot Creator Agent.

### Shot
**Location:** `src/agents/shot_creator_agent/models/output_models.py`

**Important:** This model already exists - do not create it. Import and reuse from shot creator.

**Fields used for audio generation:**
- `shot_number` (int): Sequential shot number (used for file naming)
- `actor` (str): The voice narrator/actor name (maps to voice template file in `voices/{actor}.wav`)
- `temperature` (float): Emotional intensity of the speech (0.1 - 1.0)
  - Low (0.1-0.3): Calm, neutral speech
  - Medium (0.4-0.7): Normal conversational tone
  - High (0.8-1.0): Intense, emotional delivery
- `language` (str): Language code (always "en" for English)
- `exaggeration` (float): Dramaticness/intensity level (0.1 - 1.0)
  - Low (0.1-0.3): Subtle, understated
  - Medium (0.4-0.7): Normal emphasis
  - High (0.8-1.0): Very dramatic, exaggerated
- `cfg_weight` (float): Style adherence to voice prompt (0.6 - 0.8)
  - Lower values: More variation, creative interpretation
  - Higher values: Stricter adherence to voice characteristics
- `text` (str): The actual narration or dialogue text to synthesize

**Fields not used by audio generator:**
- `reference` (str): Story location reference
- `camera_zoom` (CameraZoom): Camera framing
- `camera_angle` (CameraAngle): Camera position
- `player_actions` (str): Player instructions
- `backdrop` (str): Scene description
- `duration_seconds` (float): Shot duration

### Voice Template Requirements
- **Location**: `voices/{actor}.wav` (case-insensitive)
- **Format**: WAV audio file
- **Duration**: Recommended 3-10 seconds of clear speech
- Voice templates serve as reference for TTS voice cloning

## The Agent Workflow
The Audio Generator uses a validation-first approach to ensure all voice templates are available before starting audio generation.

```mermaid
flowchart TB
    Start[Start] --> LoadShots
    LoadShots[Load Shots JSON] --> IdentifyActors
    IdentifyActors[Identify Unique Actors] --> CheckTemplates
    CheckTemplates{All Voice Templates Present?} --> |No| ListMissing
    ListMissing[List Missing Actors] --> End[Stop]
    CheckTemplates --> |Yes| InitializeIndex
    InitializeIndex[Initialize shot_index = 0] --> CheckHasMore{Has More Shots?}
    CheckHasMore --> |Yes| GetShot
    CheckHasMore --> |No| Complete[Complete]
    GetShot[Get Shot at shot_index] --> CheckAudioExists{Audio File Exists?}
    CheckAudioExists --> |Yes| IncrementIndex
    CheckAudioExists --> |No| PreProcess
    PreProcess[Pre-process Text] --> GenerateAudio
    GenerateAudio[Generate Audio via Chatterbox TTS] --> PostProcess
    PostProcess[Post-process Audio] --> SaveAudio
    SaveAudio[Save as shot_{number}_{actor}.wav] --> IncrementIndex
    IncrementIndex[Increment shot_index] --> CheckHasMore
```

## State Management
The workflow maintains state across nodes using a session object that tracks audio generation progress.

### AudioGeneratorSession
- It is a dataclass
- Has the following properties:
    - `subject` (str): The story subject (for file paths)
    - `shots` (list[Shot]): All loaded shots from shots.json
    - `actors` (list[str]): Unique list of actor names extracted from shots
    - `shot_index` (int): Current shot being processed (0-based index)
    - `current_shot` (Shot | None): Current shot object being processed
    - `preprocessed_text` (str | None): Text after preprocessing for TTS
    - `raw_audio` (Tensor | None): Raw audio output from Chatterbox TTS
    - `processed_audio` (Tensor | None): Audio after post-processing
- This class is passed to each node as described in the guide

### Shot Reference
The Shot model is documented in detail in the "Input Data Model" section above. It is imported from `src/agents/shot_creator_agent/models/output_models.py`.

## The Nodes

### LoadShots
- Reads shots from `output/{subject}/shots.json`
- The `subject` is passed during graph invocation
- Parses the JSON into a list of `Shot` objects
- Stores `subject` and `shots` in `AudioGeneratorSession`
- If the file does not exist, End the workflow with proper error message
- If the file exists, transfer control to `IdentifyActors`

### IdentifyActors
- Iterates through all shots in `AudioGeneratorSession.shots`
- Extracts unique actor names into a set
- Stores the unique actors list in `AudioGeneratorSession.actors`
- Transfer control to `CheckTemplates`

### CheckTemplates
- For each actor in `AudioGeneratorSession.actors`:
    - Check if voice template file exists at `voices/{actor}.wav`
    - Track missing actors in a list
- If any actors are missing templates:
    - Transfer control to `ListMissing`
- If all templates exist:
    - Transfer control to `InitializeIndex`

### ListMissing
- Prints error message listing all actors without voice templates
- Example: "Missing voice templates for: Sarephine, Landra, Kelantir"
- Provides guidance: "Please create voice templates manually for these actors"
- End the workflow with error status

### InitializeIndex
- Sets `AudioGeneratorSession.shot_index = 0`
- Creates output directory `output/{subject}/audio/` if it doesn't exist
- Transfer control to `CheckHasMore`

### CheckHasMore
- Checks if `shot_index < len(shots)`
- If yes, transfer control to `GetShot`
- If no, transfer control to `Complete` (End)

### GetShot
- Retrieves shot at `shots[shot_index]`
- Stores current shot in `AudioGeneratorSession.current_shot`
- Transfer control to `CheckAudioExists`

### CheckAudioExists
- Constructs expected audio file path: `output/{subject}/audio/shot_{shot_number}_{actor}.wav`
- Checks if audio file already exists
- If file exists:
    - Prints skip message: "Skipping shot {shot_number} (audio exists)"
    - Transfer control to `IncrementIndex`
- If file does not exist:
    - Transfer control to `PreProcess`

### PreProcess
- Gets text from `current_shot.text`
- Applies text preprocessing based on Chatterbox requirements:
    - Remove special characters that cause TTS issues
    - Normalize whitespace
    - Handle quotation marks
    - Clean up punctuation
- Stores preprocessed text in `AudioGeneratorSession.preprocessed_text`
- Transfer control to `GenerateAudio`

### GenerateAudio
- Loads voice template from `voices/{current_shot.actor}.wav`
- Invokes Chatterbox TTS synthesizer with:
    - `text`: preprocessed text from session
    - `voice_path`: path to actor's voice template
    - `temperature`: `current_shot.temperature`
    - `exaggeration`: `current_shot.exaggeration`
    - `cfg_weight`: `current_shot.cfg_weight`
    - `language`: `current_shot.language`
- Receives raw audio tensor from synthesizer
- Stores raw audio in `AudioGeneratorSession.raw_audio`
- Transfer control to `PostProcess`

### PostProcess
- Gets raw audio from `AudioGeneratorSession.raw_audio`
- Applies audio post-processing based on Chatterbox requirements:
    - Normalize audio levels
    - Apply any required audio filters
    - Convert to proper WAV format (24kHz sample rate)
- Stores processed audio in `AudioGeneratorSession.processed_audio`
- Transfer control to `SaveAudio`

### SaveAudio
- Constructs output path: `output/{subject}/audio/shot_{shot_number}_{actor}.wav`
- Saves processed audio to the file
- Prints success message: "Generated audio for shot {shot_number} - {actor}"
- Transfer control to `IncrementIndex`

### IncrementIndex
- Increments `AudioGeneratorSession.shot_index` by 1
- Prints progress: "Progress: {shot_index}/{total_shots} audio files generated"
- Transfer control to `CheckHasMore`

## Chatterbox TTS Integration

The audio generation uses Chatterbox, a high-quality TTS engine. Implementation details from the chatterbox_audio_agent project.

### Chatterbox Library Setup

**Import Requirements:**
```python
import torch
import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
```

**Model Initialization:**
```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ChatterboxTTS.from_pretrained(device=device)
sample_rate = model.sr  # Should be 24000
```

### Text Preprocessing

Text preprocessing is critical for avoiding TTS audio artifacts and awkward speech splits.

**Implementation Location:** `src/libs/audio/chatterbox_utils.py`

**Implementation:**
```python
import re

def preprocess_text(text: str) -> str:
    """Preprocess text by removing or replacing special characters for TTS compatibility.

    Args:
        text: Input text to preprocess.

    Returns:
        Preprocessed text with special characters handled and TTS-friendly punctuation.
    """
    if not text:
        return text

    replacements = {
        "—": ", ",   # Em dash to comma for lighter pause
        "'": "'",    # Curly single quote to straight quote
        "…": "...",  # Ellipsis to three dots
        "*": ""      # Remove asterisks
    }

    for old, new in replacements.items():
        text = text.replace(old, new)

    # Replace semicolons before capitalized words with commas
    # This prevents unnatural TTS splits at proper nouns
    text = re.sub(r';\s+([A-Z])', r', \1', text)

    return text.strip()
```

**TTS Punctuation Rules:**
- **Em dashes (—)**: Converted to commas for lighter pauses (not semicolons which cause hard breaks)
- **Semicolons before capitals**: Replaced with commas to prevent awkward splits at proper nouns
- **Example issue**: "Drive off the Defias Thugs; we cannot" would split awkwardly at "Defias" / "Thugs"
- **Fix**: "Drive off the Defias Thugs, we cannot" flows naturally without split

**Why This Matters:**
Neural TTS engines interpret semicolons as significant prosodic boundaries with rising intonation. When followed by capitalized words (proper nouns), this creates unnatural speech splits. Converting to commas provides lighter pauses that don't break the semantic flow.

**Usage in PreProcess Node:**
- Call `preprocess_text(current_shot.text)`
- Store result in `AudioGeneratorSession.preprocessed_text`

### Audio Generation

**Implementation for GenerateAudio Node:**
```python
# Load voice template path (case-insensitive search in voices/ folder)
voice_path = f"voices/{current_shot.actor}.wav"  # Implement case-insensitive file search

# Preprocess text
processed_text = preprocess_text(current_shot.text)

# Generate audio using Chatterbox
wav = model.generate(
    processed_text,
    audio_prompt_path=voice_path,
    temperature=current_shot.temperature,
    exaggeration=current_shot.exaggeration,
    cfg_weight=current_shot.cfg_weight
)

# Store in session
session.raw_audio = wav
```

**Key Points:**
- The `model.generate()` method returns a `torch.Tensor`
- The tensor is the raw audio waveform
- Sample rate is stored in `model.sr` (24000 Hz)
- No need to pass `language` parameter (Chatterbox handles English by default)

### Audio Post-Processing

**Implementation (use exactly as-is):**
```python
def normalize_audio(audio: torch.Tensor, target_level: float = -20.0) -> torch.Tensor:
    """Normalize audio to a target level in dBFS.

    Args:
        audio: Input audio tensor.
        target_level: Target level in dBFS.

    Returns:
        Normalized audio tensor.
    """
    if audio.numel() == 0:
        return audio

    # Calculate current RMS
    rms = torch.sqrt(torch.mean(audio ** 2))

    # Avoid division by zero
    if rms < 1e-6:
        return audio

    # Calculate scaling factor
    target_linear = 10 ** (target_level / 20.0)
    current_linear = rms
    scaling = target_linear / current_linear

    # Apply scaling and clamp to prevent clipping
    return torch.clamp(audio * scaling, -0.999, 0.999)
```

**Usage in PostProcess Node:**
```python
# Get raw audio from session
raw_audio = session.raw_audio

# Normalize the audio
processed_audio = normalize_audio(raw_audio, target_level=-20.0)

# Store in session
session.processed_audio = processed_audio
```

### Audio Saving

**Implementation for SaveAudio Node:**
```python
import os
import torchaudio as ta

def save_audio(filepath: str, audio: torch.Tensor, sample_rate: int):
    """Save audio tensor to WAV file.

    Args:
        filepath: Path to save the audio file.
        audio: Audio tensor to save.
        sample_rate: Sample rate of the audio.
    """
    # Ensure output directory exists
    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)

    # Ensure audio is at least 2D (channels, samples)
    if len(audio.shape) == 1:
        audio = audio.unsqueeze(0)  # Convert to mono

    # Save the audio
    ta.save(filepath, audio, sample_rate)
```

**Usage:**
```python
# Construct output path
output_path = f"output/{subject}/audio/shot_{current_shot.shot_number}_{current_shot.actor}.wav"

# Save processed audio
save_audio(output_path, session.processed_audio, sample_rate)
```

### TTS Parameters Mapping

| Shot Field | Chatterbox Parameter | Description |
|------------|---------------------|-------------|
| `text` | First argument (after preprocessing) | The text to synthesize |
| `actor` | `audio_prompt_path` | Path to voice template: `voices/{actor}.wav` |
| `temperature` | `temperature` | Emotional intensity (0.1 - 1.0) |
| `exaggeration` | `exaggeration` | Dramaticness level (0.1 - 1.0) |
| `cfg_weight` | `cfg_weight` | Style adherence (0.6 - 0.8) |

**Note:** The `language` field from Shot is not used - Chatterbox defaults to English.

### Output File Structure

Audio files are saved with this naming pattern:
- Pattern: `shot_{shot_number}_{actor}.wav`
- Location: `output/{subject}/audio/`
- Examples:
  - `shot_1_aaryan.wav`
  - `shot_2_aaryan.wav`
  - `shot_3_Sarephine.wav`

### Error Handling

**Common Issues:**
- Voice template not found → Caught in CheckTemplates node (stops workflow)
- TTS synthesis fails → Log error, skip shot, continue to next
- Audio save fails → Log error, continue (will retry on next run)

**Recovery Strategy:**
- If audio file generation fails, the file won't exist
- CheckAudioExists will detect missing file on next run
- Failed shots will be retried automatically

## Graph Entry Point
Graph entry point should be set up in two steps:
1. Define the Graph object based on the section "## Invoking A Graph" of the Pydantic Graph blueprint
2. Create a CLI entry point to invoke the graph
3. The `subject` should be passed to the graph and set to the first node

## CLI Interface

Create `src/ui/cli/create_audio.py` following the pattern from `create_shots.py`:

**Features:**
- Accepts `--subject` or `-s` argument (required)
- Validates that `output/{subject}/shots.json` exists before starting
- If shots.json missing, provides helpful error message directing user to run create_shots first
- Displays start message with subject and shots file path
- Invokes AudioGeneratorGraph with the subject
- Displays completion message with output directory path
- Proper error handling with traceback display
- Exit codes: 0 for success, 1 for errors

**Example usage:**
```bash
python -m src.ui.cli.create_audio --subject shadowglen
```

**Expected output structure:**
```
output/{subject}/
├── shots.json         (input: generated by shot creator)
└── audio/
    ├── shot_1_aaryan.wav
    ├── shot_2_aaryan.wav
    ├── shot_3_Sarephine.wav
    └── ...
```

## Batch File

Create `start_audio_generator.bat` for easy CLI access:
```batch
@echo off
echo ========================================
echo Audio Generator
echo ========================================
echo.

if "%1"=="" (
    echo Usage: start_audio_generator.bat [subject]
    echo.
    echo Example:
    echo   start_audio_generator.bat shadowglen
    echo.
    pause
    exit /b 1
)

python -m src.ui.cli.create_audio --subject %1
pause
```

**Usage:**
```bash
start_audio_generator shadowglen
```

## Error Handling
- For any kind of error, keep it KISS for now
- If shots.json doesn't exist, end gracefully with error message
- If voice template missing for any actor, list all missing actors and end gracefully
- If audio file already exists, skip generation and continue to next shot
- If Chatterbox TTS fails, log error but continue to next shot (allows partial recovery)
- If audio save fails, log error but continue (will retry on next run since file won't exist)
